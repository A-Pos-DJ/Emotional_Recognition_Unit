{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotional_Recognition_Unit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "68iGIna3uAs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check to see if the first few initialization blocks have already been ran\n",
        "try: \n",
        "  init\n",
        "  if init is True:\n",
        "    print(\"Program has been pre-initialized, skipping initialization...\")\n",
        "  else:\n",
        "    print(\"Initializing program...\")\n",
        "    init = False\n",
        "except NameError:\n",
        "  print(\"Initializing program...\")\n",
        "  init = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsO-rQKeDcBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if init is not True:\n",
        "  #Set up python virtual enviroment for EmoPy\n",
        "  !pip install graphviz\n",
        "  !sudo pip install virtualenv\n",
        "  !sudo apt-get install python3.6-venv\n",
        "  !python3.6 -m venv venv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwhKCS3396H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if init is not True:\n",
        "  #Import EmoPy module and github project\n",
        "  !pip install EmoPy\n",
        "  !git clone https://github.com/thoughtworksarts/EmoPy.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQ_5qlSBRZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change the directory to the imported EmoPy github\n",
        "cd /content/EmoPy/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNjmAJ4HBUKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if init is not True:\n",
        "  #Run installations for compatable versions of python modules for\n",
        "  # EmoPy to run with no errors\n",
        "  !pip install Keras==2.2.4\n",
        "  !pip install scipy==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wP6tVC9Cv6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run all EmoPy tests - this cell can be deleted once we are sure EmoPy runs with no issues\n",
        "#!python3 EmoPy/tests/run_all.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_RCeynRxkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change the directory to the example models\n",
        "cd EmoPy/examples/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz_b-TTapVle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Program Variable and Method Inits\n",
        "from google.colab import drive\n",
        "from keras.models import model_from_json\n",
        "\n",
        "#Program Variables\n",
        "#programRunning = True\n",
        "#menuDisplayed = False\n",
        "global triedToRunAgain\n",
        "triedToRunAgain = False\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "modelsFilePath = F\"/content/gdrive/My Drive/Colab_Notebooks/Emotional_Recognition_Unit/TrainedModels/\"\n",
        "\n",
        "\n",
        "emotionalFERSubset1 = ['surprise', 'fear', 'anger', 'calm']\n",
        "emotionalFERSubset2 = ['surprise', 'disgust', 'happiness']\n",
        "emotionalFERSubset3 = ['surprise', 'fear', 'anger']\n",
        "emotionalFERSubset4 = ['fear', 'anger', 'calm']\n",
        "emotionalFERSubset5 = ['calm', 'anger', 'happiness']\n",
        "emotionalFERSubset6 = ['fear', 'disgust', 'anger']\n",
        "emotionalFERSubset7 = ['surprise', 'disgust', 'calm']\n",
        "emotionalFERSubset8 = ['sadness', 'disgust', 'surprise']\n",
        "emotionalFERSubset9 = ['anger', 'happiness']\n",
        "\n",
        "global convModel\n",
        "cModelFileName = 'Conv/ConvTrainedModel.json'\n",
        "cWeightsFileName = 'Conv/ConvWeights.h5'\n",
        "cMapFileName = 'Conv/ConvMap.json'\n",
        "cGraphFileName = 'Conv/ConvModelGraph.png'\n",
        "\n",
        "\n",
        "#Program Methods\n",
        "def incrementPhotosTaken():\n",
        "  numPhotosTaken += 1\n",
        "\n",
        "#load a trained model\n",
        "def LoadModel(modelNameArg, modelWeightsNameArg):\n",
        "  loadedModel = None\n",
        "\n",
        "  try:\n",
        "    # load json and create model\n",
        "    json_file = open(modelsFilePath+modelNameArg, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loadedModel = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loadedModel.load_weights(modelsFilePath+modelWeightsNameArg)\n",
        "    print(\"Sucessfully loaded model from Google drive...\")\n",
        "    return loadedModel\n",
        "\n",
        "  except:\n",
        "    print(\"Failed to load model at \" + modelsFilePath + modelNameArg)\n",
        "    return None\n",
        "\n",
        "\n",
        "#Set the initializer to be true\n",
        "init = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SucxddsPhOmj",
        "colab": {}
      },
      "source": [
        "#Javascript webcam method that outputs base64 image data\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_single_photo(filename='testPhoto.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function take_single_photo(quality) {\n",
        "      //create a button element to take a photo\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      //add the video recording element to the display\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      //add the buttons and video to the diaplay\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for a button to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();    \n",
        "      dataURL = canvas.toDataURL('image/jpeg', quality);\n",
        "      return dataURL.replace(/^data:image\\/(png|jpg);base64,/, \"\");\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  data = eval_js('take_single_photo({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "buJCl90WhNfq",
        "colab": {}
      },
      "source": [
        "#EmoPy FERModel prediction and menu\n",
        "from EmoPy.src.fermodel import FERModel\n",
        "from IPython.display import Image\n",
        "\n",
        "#All menu options and what they do for the FER Model menu\n",
        "def FERMenuOption(b):\n",
        "  #with output:\n",
        "  if b.description == \"FER Set Descriptions\":\n",
        "    print(\"FERModel tests images for emotion sets, Tested emotions are divided up into sets:\")\n",
        "    print(\"FER Set 1:\")\n",
        "    print(emotionalFERSubset1)\n",
        "    print(\"FER Set 2:\")\n",
        "    print(emotionalFERSubset2)\n",
        "    print(\"FER Set 3:\")\n",
        "    print(emotionalFERSubset3)\n",
        "    print(\"FER Set 4:\")\n",
        "    print(emotionalFERSubset4)\n",
        "    print(\"FER Set 5:\")\n",
        "    print(emotionalFERSubset5)\n",
        "    print(\"FER Set 6:\")\n",
        "    print(emotionalFERSubset6)\n",
        "    print(\"FER Set 7:\")\n",
        "    print(emotionalFERSubset7)\n",
        "    print(\"FER Set 8:\")\n",
        "    print(emotionalFERSubset8)\n",
        "    print(\"FER Set 9:\")\n",
        "    print(emotionalFERSubset9)\n",
        "  elif b.description == \"FER Set 1\":\n",
        "    print(\"Selecting the FER set1\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset1)\n",
        "  elif b.description == \"FER Set 2\":\n",
        "    print(\"Selecting the FER set2\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset2)\n",
        "  elif b.description == \"FER Set 3\":\n",
        "    print(\"Selecting the FER set3\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset3)\n",
        "  elif b.description == \"FER Set 4\":\n",
        "    print(\"Selecting the FER set4\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset4)\n",
        "  elif b.description == \"FER Set 5\":\n",
        "    print(\"Selecting the FER set5\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset5)\n",
        "  elif b.description == \"FER Set 6\":\n",
        "    print(\"Selecting the FER set6\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset6)\n",
        "  elif b.description == \"FER Set 7\":\n",
        "    print(\"Selecting the FER set7\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset7)\n",
        "  elif b.description == \"FER Set 8\":\n",
        "    print(\"Selecting the FER set8\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset8)\n",
        "  elif b.description == \"FER Set 9\":\n",
        "    print(\"Selecting the FER set9\")\n",
        "    PhotoMode(\"FER Single\", emotionalFERSubset9)\n",
        "  elif b.description == \"All FER Sets\":\n",
        "    print(\"Selecting all FER sets\")\n",
        "    PhotoMode(\"FER All\")\n",
        "\n",
        "\n",
        "# Prediction model for Base64 Photo Data\n",
        "def FERModelPredictionSingleSet(base64ImageDataArg, emotionSubsetArg):\n",
        "  target_emotions = emotionSubsetArg\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "\n",
        "  #make a prediction based on the image that was just taken from the webcam\n",
        "  print('Predicting emotions based on the following webcam image...')\n",
        "  display(Image(base64ImageDataArg))\n",
        "  print('Saved to {}'.format(base64ImageDataArg))\n",
        "  model.predict(base64ImageDataArg)\n",
        "  return\n",
        "\n",
        "\n",
        "# Prediction model for Base64 Photo Data\n",
        "def FERModelPredictionAllSets(base64ImageDataArg):\n",
        "  #make a prediction based on the image that was just taken from the webcam\n",
        "  print('Predicting emotions based on the following webcam image...')\n",
        "  display(Image(base64ImageDataArg))\n",
        "  print('Saved to {}'.format(base64ImageDataArg))\n",
        "\n",
        "  print('FER Set 1:')\n",
        "  target_emotions = emotionalFERSubset1\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 2:')\n",
        "  target_emotions = emotionalFERSubset2\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 3:')\n",
        "  target_emotions = emotionalFERSubset3\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 4:')\n",
        "  target_emotions = emotionalFERSubset4\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 5:')\n",
        "  target_emotions = emotionalFERSubset5\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 6:')\n",
        "  target_emotions = emotionalFERSubset6\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 7:')\n",
        "  target_emotions = emotionalFERSubset7\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 8:')\n",
        "  target_emotions = emotionalFERSubset8\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "\n",
        "  print('FER Set 9:')\n",
        "  target_emotions = emotionalFERSubset9\n",
        "  model = FERModel(target_emotions, verbose=True)\n",
        "  model.predict(base64ImageDataArg)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJk8NQQeLJ7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convolutional model\n",
        "import warnings\n",
        "import sys\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "import scipy\n",
        "from skimage.io import imread\n",
        "\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n",
        "#All menu options and what they do for the convolutional menu\n",
        "def ConvMenuOption(b):\n",
        "  #with output:\n",
        "  if b.description == \"Conv Model Descriptions\":\n",
        "    print(\"Convolutional model is first trained and then used to classify images:\")\n",
        "    print(\"Load Model:\")\n",
        "    print(\"Loads a trained convolutional model from your google drive to this runtime\")\n",
        "    print(\"Classify Image:\")\n",
        "    print(\"Use the trained model to classfy images based on emotion\")\n",
        "  elif b.description == \"Load Model\":\n",
        "    print(\"Loading a new convolutional model:\")\n",
        "    global convModel\n",
        "    convModel = LoadModel(cModelFileName, cWeightsFileName)\n",
        "  elif b.description == \"Classify Image\":\n",
        "    if convModel == None:\n",
        "      print(\"Your convolutional model needs to be loaded first!\")\n",
        "    else:\n",
        "      print(\"Classifying with trained convolutional model:\")\n",
        "      PhotoMode(\"Conv\")\n",
        "\n",
        "\n",
        "# Prediction model for Base64 Photo Data\n",
        "def ConvModelPrediction(base64ImageDataArg):\n",
        "  #make a prediction based on the image that was just taken from the webcam\n",
        "  print('Predicting emotions based on the following webcam image...')\n",
        "  display(Image(base64ImageDataArg))\n",
        "  print('Saved to {}'.format(base64ImageDataArg))\n",
        "\n",
        "  #Still in the process of getting a loaded model to make a prediction\n",
        "\n",
        "  #tk = Tokenizer()\n",
        "  #tk.fit_on_texts(base64ImageDataArg)\n",
        "  #index_list = tk.texts_to_sequences(base64ImageDataArg)\n",
        "  #x_train = pad_sequences(index_list, maxlen=10000)\n",
        "\n",
        "  #nparr = np.fromstring(base64.b64decode(base64ImageDataArg), np.uint8)\n",
        "  #print(nparr)\n",
        "  #img2 = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
        "  #print(img2)\n",
        "\n",
        "  #img2 = np.reshape(img2, (0, 58, 58, 10))\n",
        "\n",
        "  \n",
        "  im = imread(Image(base64ImageDataArg).data)\n",
        "  #im = scipy.imread(Image(base64ImageDataArg).data)\n",
        "  indices = np.dstack(np.indices(im.shape[:2]))\n",
        "  data = np.concatenate((im, indices), axis=-1)\n",
        "\n",
        "  #t = np.arange(25, dtype=np.float64)\n",
        "  #s = base64.b64encode(t)\n",
        "  #r = base64.decodebytes(s)\n",
        "  #q = np.frombuffer(r, dtype=np.float64)\n",
        "  #q = np.reshape(q,(64,64))\n",
        "  #img2 = np.reshape(q, (0, 58, 58, 10))\n",
        "\n",
        "  print(im)\n",
        "\n",
        "  try:\n",
        "    convModel.predict(im)\n",
        "  except Exception as err:\n",
        "    print(\"Error with convolution prediction:\")\n",
        "    print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAgZ-h58OdYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generic Photo Mode that can be used by all models\n",
        "def PhotoMode(predictionType, emotionalSubsetArg=None):\n",
        "  global triedToRunAgain\n",
        "\n",
        "  try:\n",
        "    if predictionType != \"MultiPhoto\":\n",
        "      base64ImageData = take_single_photo()\n",
        "      if predictionType == \"FER Single\":\n",
        "        FERModelPredictionSingleSet(base64ImageData, emotionalSubsetArg)\n",
        "      elif predictionType == \"FER All\":\n",
        "        FERModelPredictionAllSets(base64ImageData)\n",
        "      elif predictionType == \"Conv\":\n",
        "        ConvModelPrediction(base64ImageData)\n",
        "\n",
        "  except Exception as err:\n",
        "    global triedToRunAgain\n",
        "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "    # grant the page permission to access it.\n",
        "    print(str(err))\n",
        "\n",
        "    #Give the function a second attempt if there has not already one been given\n",
        "    if triedToRunAgain == False:\n",
        "      print(\"Trying Again...\")\n",
        "      PhotoMode(predictionType, emotionalSubsetArg)\n",
        "      triedToRunAgain = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_REnCzBqAGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generic menu that can be used by all\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def CreateMenu(menuOptionListArg, menuHeaderStringArg, menuTypeArg):\n",
        "  menuButtons = []\n",
        "\n",
        "  for menuOption in menuOptionListArg:\n",
        "    AddMenuButton(menuOption, menuButtons)\n",
        "  DisplayMenu(menuHeaderStringArg, menuButtons, menuTypeArg)\n",
        "\n",
        "def AddMenuButton(textStringArg, menuButtonListArg):\n",
        "  menuButton = widgets.Button(description=textStringArg)\n",
        "  menuButtonListArg.append(menuButton)\n",
        "\n",
        "def DisplayMenu(menuHeaderStringArg, menuButtonListArg, menuTypeArg):\n",
        "  menuOutput = widgets.Output()\n",
        "  print(menuHeaderStringArg)\n",
        "  for button in menuButtonListArg:\n",
        "    display(button)\n",
        "  display(menuOutput)\n",
        "\n",
        "  #Differentiate the menu types according to which function they call\n",
        "  if menuTypeArg is \"Main\":  \n",
        "    for button in menuButtonListArg:\n",
        "      button.on_click(MainMenuOption)\n",
        "  if menuTypeArg is \"FER\":  \n",
        "    for button in menuButtonListArg:\n",
        "      button.on_click(FERMenuOption)\n",
        "  if menuTypeArg is \"Conv\":\n",
        "    for button in menuButtonListArg:\n",
        "      button.on_click(ConvMenuOption)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY54ARlSjnkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Main Menu\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "#All menu options and what they do for the main menu\n",
        "def MainMenuOption(b):\n",
        "  #with output:\n",
        "  if b.description == \"FERModel\":\n",
        "    print(\"Starting FERModel mode:\")\n",
        "    fMenuOptions = [\"FER Set Descriptions\", \"FER Set 1\", \"FER Set 2\", \"FER Set 3\",\\\n",
        "                    \"FER Set 4\", \"FER Set 5\", \"FER Set 6\", \"FER Set 7\", \"FER Set 8\",\\\n",
        "                    \"FER Set 9\", \"All FER Sets\"]\n",
        "    fMenuHeader = \"Choose which emotional set you would like to use for image prediction:\"\n",
        "    fMenuType = \"FER\"\n",
        "    CreateMenu(fMenuOptions, fMenuHeader, fMenuType)\n",
        "      \n",
        "  elif b.description == \"ConvModel\":\n",
        "    print(\"Starting Conv Model mode:\")\n",
        "    cMenuOptions = [\"Conv Model Descriptions\", \"Load Model\", \"Classify Image\"]\n",
        "    cMenuHeader = \"Choose which convolutional model option you would like to select:\"\n",
        "    cMenuType = \"Conv\"\n",
        "    CreateMenu(cMenuOptions, cMenuHeader, cMenuType)\n",
        "    \n",
        "  elif b.description == \"Exit\":\n",
        "    print(\"This feature has not been implemented yet...\")\n",
        "\n",
        "#Create the main menu and run it\n",
        "menuOptions = [\"FERModel\", \"ConvModel\", \"Exit\"]\n",
        "menuHeader = \"Choose a menu option:\"\n",
        "menuType = \"Main\"\n",
        "CreateMenu(menuOptions, menuHeader, menuType)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wirAh3-v98qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To clear output, you will need to import this method and use it\n",
        "#from IPython.display import clear_output\n",
        "#clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}